{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dafbb4c",
   "metadata": {},
   "source": [
    "## Text Preprocessing 1 (Cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8542d",
   "metadata": {},
   "source": [
    "#### 1) Tokenization:\n",
    "    * Takes the sentence and converting it into words.\n",
    "\n",
    "#### ^) Stop words\n",
    "\n",
    "#### 2) Stemming:\n",
    "    * Process of reducing words to their Base word stem.\n",
    "    * We'll be trying to find the base stem of the word.\n",
    "    Eg: historical , History --stemming--> histori (This word won't have any meaning in some case it may have meaning )\n",
    "    * In short we are getting root word/base form. \n",
    "\n",
    "    Pro - Stemming is really fast | Cons - It is removing the meaning of the word\n",
    "\n",
    "    > Use case:\n",
    "        1) Spam classifier\n",
    "        2) Review classifier\n",
    "\n",
    "#### 3) Lemmatization:\n",
    "    * To overcome the con in stemming.\n",
    "    * historical , History --Lemmatization--> history (Meaningful word)\n",
    "    * Identify the meaningful base word from dict\n",
    "\n",
    "    Pro - Meaningful word | Cons - It is slow as it searches the dictionary for words.\n",
    "\n",
    "    > Use case:\n",
    "        1) Text Summarization\n",
    "        2) Language translation\n",
    "        3) Chat bot\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a2966",
   "metadata": {},
   "source": [
    "## Text preprocessing 2 (Convert words to vector)\n",
    "\n",
    "[OHE(One Hot Encoding), Bag of Words, TD-IDF(Term Frequency-Inverse Document Frequency), Word2Vector]\n",
    "\n",
    "#### Basic Terminologies used in NLP:\n",
    "    1) CORPUS --> paragraph\n",
    "    2) Documents --> sentence\n",
    "    3) Vocabulary --> Unique words in para/sentence\n",
    "    4) Words\n",
    "\n",
    "### 1) Bag of Words:\n",
    "\n",
    "    D1 -> He is a good boy\n",
    "    D2 -> She is a good girl\n",
    "    D3 -> Boy and girl are good\n",
    "\n",
    "    * Using Stop word remove the umimportant words like [is, are, he...]. Make sure all the words are conveted to small case\n",
    "\n",
    "        Note: Unimportance words are applicable only in sentiment classification, toxic classification. In someother cases it would be important.\n",
    "\n",
    "    D1 -> good boy\n",
    "    D2 -> good girl\n",
    "    D3 -> boy girl good.\n",
    "\n",
    "    * Identify the Vocabularies and its frequency\n",
    "\n",
    "        Vocabulary | Frequeny\n",
    "        \n",
    "        good            3\n",
    "        boy             2\n",
    "        girl            2\n",
    "\n",
    "    * convert to feature.features\n",
    "        From the docs after using stop word, increase the count of the word in f (feature) and make the docu to a vector\n",
    "\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  1          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1  \n",
    "\n",
    "        --> if in case D1 is like below\n",
    "            D1 -> He is a good good boy --> good good boy\n",
    "\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  2          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1  \n",
    "\n",
    "        --> In binary BoW we only use 1s and 0s, so our vectors will be like, it doc should have the word, if it has it will be 1 esle 0.\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  1          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1 \n",
    "\n",
    "    Advantages:\n",
    "        1) Simple and Intence\n",
    "\n",
    "    DisAdv:\n",
    "        1) Sparcity - Matrixs 1XN - N may increase based on no of Words.\n",
    "        2) OOV (out of vocabulary) - If a new Word is introduced in any of doc it can't handle.\n",
    "        3) Ordering has completly changed\n",
    "        4) Sematic meaning is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c051b9e",
   "metadata": {},
   "source": [
    "In BoW we can capture the simatic meaning, inorder to capture the sementic meaning we can go with N-Grams.\n",
    "\n",
    "### N-Grams\n",
    "    * Bigrams, Trigrams, ... N gram\n",
    "\n",
    "#### Bi-gram and Tri Gram.\n",
    "\n",
    "    Let's take the below docs\n",
    "    D1 -> good boy\n",
    "    D2 -> good girl\n",
    "    D3 -> boy girl good.\n",
    "\n",
    "        f1     | f2    | f3     | f4            | f5\n",
    "         good     boy     girl  good boy        good girl      \n",
    "     D1  1          1       0       1               0\n",
    "     D2  1          0       1       0               1\n",
    "     D3  1          1       1       0               0\n",
    "\n",
    "        f1,f2,f3 - Uni Gram\n",
    "        f4, f3 - Bi-Gram\n",
    "     * How do we take the bigrams?\n",
    "\n",
    "        Eg: \n",
    "            1) I am suganth -> Has 2 Bi-Grams\n",
    "\n",
    "                I am , am suganth\n",
    "            \n",
    "            2) I am not feeling well -> Has 3 Tri Grams\n",
    "\n",
    "                I am not, am not feeling, not feeling well.\n",
    "            \n",
    "        from the above example we can observ and see it is creating sematic meaning.\n",
    "\n",
    "#### Representation of N grams:\n",
    "\n",
    "    (1,3) -> we will create the features from unigram to trigram.\n",
    "\n",
    "    Eg:\n",
    "\n",
    "    D1 - I am not feeling well\n",
    "\n",
    "    D1  f1|     f2|     f3|     f4| .....  f6|    f7|......fn\n",
    "        I       am      not     feeling    I am   am not..not feeling well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d06abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c56c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nReviewers say 'Breaking Bad' is celebrated for its intricate storytelling, \\ncomplex characters, and moral ambiguity. Walter White's transformation captivates \\naudiences, praised for meticulous writing and compelling arcs. Bryan Cranston \\nand Aaron Paul deliver standout performances. Cinematography, dark humor, and \\nexploration of human nature are frequently highlighted. Often compared to \\n'The Sopranos' and 'The Wire', it is considered a modern TV masterpiece. \\nMinor flaws are noted, but overall quality is exceptional.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Reviewers say 'Breaking Bad' is celebrated for its intricate storytelling, \n",
    "complex characters, and moral ambiguity. Walter White's transformation captivates \n",
    "audiences, praised for meticulous writing and compelling arcs. Bryan Cranston \n",
    "and Aaron Paul deliver standout performances. Cinematography, dark humor, and \n",
    "exploration of human nature are frequently highlighted. Often compared to \n",
    "'The Sopranos' and 'The Wire', it is considered a modern TV masterpiece. \n",
    "Minor flaws are noted, but overall quality is exceptional.\n",
    "\"\"\"\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb79eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c91efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suganth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## tokenization-- Convert paragraph->sentences->words\n",
    "\n",
    "nltk.download('punkt')\n",
    "sentences=nltk.sent_tokenize(paragraph) # inorder to use it we need to download punkt age in nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ab0d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "stemmer=PorterStemmer()\n",
    "stemmer.stem('history')\n",
    "#stemmer.stem('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e23667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suganth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2417f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3e21c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' reviewers say  breaking bad  is celebrated for its intricate storytelling   complex characters  and moral ambiguity ',\n",
       " 'walter white s transformation captivates  audiences  praised for meticulous writing and compelling arcs ',\n",
       " 'bryan cranston  and aaron paul deliver standout performances ',\n",
       " 'cinematography  dark humor  and  exploration of human nature are frequently highlighted ',\n",
       " 'often compared to   the sopranos  and  the wire   it is considered a modern tv masterpiece ',\n",
       " 'minor flaws are noted  but overall quality is exceptional ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f95416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suganth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##English stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d47614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review\n",
      "say\n",
      "break\n",
      "bad\n",
      "celebr\n",
      "intric\n",
      "storytel\n",
      "complex\n",
      "charact\n",
      "moral\n",
      "ambigu\n",
      "walter\n",
      "white\n",
      "transform\n",
      "captiv\n",
      "audienc\n",
      "prais\n",
      "meticul\n",
      "write\n",
      "compel\n",
      "arc\n",
      "bryan\n",
      "cranston\n",
      "aaron\n",
      "paul\n",
      "deliv\n",
      "standout\n",
      "perform\n",
      "cinematographi\n",
      "dark\n",
      "humor\n",
      "explor\n",
      "human\n",
      "natur\n",
      "frequent\n",
      "highlight\n",
      "often\n",
      "compar\n",
      "soprano\n",
      "wire\n",
      "consid\n",
      "modern\n",
      "tv\n",
      "masterpiec\n",
      "minor\n",
      "flaw\n",
      "note\n",
      "overal\n",
      "qualiti\n",
      "except\n"
     ]
    }
   ],
   "source": [
    "## stemming\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        #print(word)\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "815ef51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewer\n",
      "say\n",
      "breaking\n",
      "bad\n",
      "celebrated\n",
      "intricate\n",
      "storytelling\n",
      "complex\n",
      "character\n",
      "moral\n",
      "ambiguity\n",
      "walter\n",
      "white\n",
      "transformation\n",
      "captivates\n",
      "audience\n",
      "praised\n",
      "meticulous\n",
      "writing\n",
      "compelling\n",
      "arc\n",
      "bryan\n",
      "cranston\n",
      "aaron\n",
      "paul\n",
      "deliver\n",
      "standout\n",
      "performance\n",
      "cinematography\n",
      "dark\n",
      "humor\n",
      "exploration\n",
      "human\n",
      "nature\n",
      "frequently\n",
      "highlighted\n",
      "often\n",
      "compared\n",
      "soprano\n",
      "wire\n",
      "considered\n",
      "modern\n",
      "tv\n",
      "masterpiece\n",
      "minor\n",
      "flaw\n",
      "noted\n",
      "overall\n",
      "quality\n",
      "exceptional\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizton\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        #print(word)\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmatizer.lemmatize(word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
