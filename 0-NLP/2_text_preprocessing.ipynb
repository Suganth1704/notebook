{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dafbb4c",
   "metadata": {},
   "source": [
    "## Text Preprocessing 1 (Cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8542d",
   "metadata": {},
   "source": [
    "#### 1) Tokenization:\n",
    "    * Takes the sentence and converting it into words.\n",
    "\n",
    "#### ^) Stop words\n",
    "\n",
    "#### 2) Stemming:\n",
    "    * Process of reducing words to their Base word stem.\n",
    "    * We'll be trying to find the base stem of the word.\n",
    "    Eg: historical , History --stemming--> histori (This word won't have any meaning in some case it may have meaning )\n",
    "    * In short we are getting root word/base form. \n",
    "\n",
    "    Pro - Stemming is really fast | Cons - It is removing the meaning of the word\n",
    "\n",
    "    > Use case:\n",
    "        1) Spam classifier\n",
    "        2) Review classifier\n",
    "\n",
    "#### 3) Lemmatization:\n",
    "    * To overcome the con in stemming.\n",
    "    * historical , History --Lemmatization--> history (Meaningful word)\n",
    "    * Identify the meaningful base word from dict\n",
    "\n",
    "    Pro - Meaningful word | Cons - It is slow as it searches the dictionary for words.\n",
    "\n",
    "    > Use case:\n",
    "        1) Text Summarization\n",
    "        2) Language translation\n",
    "        3) Chat bot\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a2966",
   "metadata": {},
   "source": [
    "## Text preprocessing 2 (Convert words to vector)\n",
    "\n",
    "[OHE(One Hot Encoding), Bag of Words, TD-IDF(Term Frequency-Inverse Document Frequency), Word2Vector]\n",
    "\n",
    "#### Basic Terminologies used in NLP:\n",
    "    1) CORPUS --> paragraph\n",
    "    2) Documents --> sentence\n",
    "    3) Vocabulary --> Unique words in para/sentence\n",
    "    4) Words\n",
    "\n",
    "### 1) Bag of Words:\n",
    "\n",
    "    D1 -> He is a good boy\n",
    "    D2 -> She is a good girl\n",
    "    D3 -> Boy and girl are good\n",
    "\n",
    "    * Using Stop word remove the umimportant words like [is, are, he...]. Make sure all the words are conveted to small case\n",
    "\n",
    "        Note: Unimportance words are applicable only in sentiment classification, toxic classification. In someother cases it would be important.\n",
    "\n",
    "    D1 -> good boy\n",
    "    D2 -> good girl\n",
    "    D3 -> boy girl good.\n",
    "\n",
    "    * Identify the Vocabularies and its frequency\n",
    "\n",
    "        Vocabulary | Frequeny\n",
    "        \n",
    "        good            3\n",
    "        boy             2\n",
    "        girl            2\n",
    "\n",
    "    * convert to feature.features\n",
    "        From the docs after using stop word, increase the count of the word in f (feature) and make the docu to a vector\n",
    "\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  1          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1  \n",
    "\n",
    "        --> if in case D1 is like below\n",
    "            D1 -> He is a good good boy --> good good boy\n",
    "\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  2          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1  \n",
    "\n",
    "        --> In binary BoW we only use 1s and 0s, so our vectors will be like, it doc should have the word, if it has it will be 1 esle 0.\n",
    "                f1     | f2    | f3\n",
    "                good     boy     girl\n",
    "            D1  1          1       0\n",
    "            D2  1          0       1\n",
    "            D3  1          1       1 \n",
    "\n",
    "    Advantages:\n",
    "        1) Simple and Intence\n",
    "\n",
    "    DisAdv:\n",
    "        1) Sparcity - Matrixs 1XN - N may increase based on no of Words.\n",
    "        2) OOV (out of vocabulary) - If a new Word is introduced in any of doc it can't handle.\n",
    "        3) Ordering has completly changed\n",
    "        4) Sematic meaning is lost."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
