{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d42896",
   "metadata": {},
   "source": [
    "# NLP for ML & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb65234",
   "metadata": {},
   "source": [
    "### Agenda:\n",
    "\n",
    "1) Roadmap of NLP\n",
    "2) Why NLP\n",
    "3) Tokenization, stemming ..\n",
    "4) Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9632ba",
   "metadata": {},
   "source": [
    "#### Why NLP?\n",
    "    * NLP -> Data set (Text) -> Model -> O/P\n",
    "    * How text is processed and given to a machine so that it can under stand.\n",
    "    * Vector play a major role in it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae131084",
   "metadata": {},
   "source": [
    "#### Roadmap of NLP\n",
    "\n",
    "    Text Preprocessing -> ML use case -> Deeplearning (RNN, LSTM RNN, GRU RNN) -> Advance text preprocessing (Word embedding) -> Advance deep learning(Bidirectional LSTM, Encoders, decoders) -> Transformer -> BERT\n",
    "\n",
    "    Text Preprocessing -> How we can clean the texts, convert text into words of vectors how a machine can understand what we say.\n",
    "     > Bag of Words, TD-IDF(Term Frequency-Inverse Document Frequency), stop word\n",
    "\n",
    "     * Text Preprocessing 1 (Cleaning) -> Tokenization (^Stopwords)> Stemming > Lemmatization\n",
    "     * Text Preprocessing 2 (Convert words to vectors) -> BoW, TFIDF, Unigrams, Bigrams\n",
    "     * Text preprocessing 3  -> Word2Vec, AugWord2Vec\n",
    "\n",
    "     Note: Each text processing overcomes the issues in it previous process.\n",
    "\n",
    "#### Libs Used\n",
    "    * NLTK\n",
    "    * spacy\n",
    "    * Text Blob\n",
    "    * Tensorflow/PyTorch\n",
    "    * Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ececd4",
   "metadata": {},
   "source": [
    "## Text Preprocessing 1 (Cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b35306",
   "metadata": {},
   "source": [
    "#### 1) Tokenization:\n",
    "    * Takes the sentence and converting it into words.\n",
    "\n",
    "#### ^) Stop words\n",
    "\n",
    "#### 2) Stemming:\n",
    "    * Process of reducing words to their Base word stem.\n",
    "    * We'll be trying to find the base stem of the word.\n",
    "    Eg: historical , History --stemming--> histori (This word won't have any meaning in some case it may have meaning )\n",
    "    * In short we are getting root word/base form. \n",
    "\n",
    "    Pro - Stemming is really fast | Cons - It is removing the meaning of the word\n",
    "\n",
    "    > Use case:\n",
    "        1) Spam classifier\n",
    "        2) Review classifier\n",
    "\n",
    "#### 3) Lemmatization:\n",
    "    * To overcome the con in stemming.\n",
    "    * historical , History --Lemmatization--> history (Meaningful word)\n",
    "    * Identify the meaningful base word from dict\n",
    "\n",
    "    Pro - Meaningful word | Cons - It is slow as it searches the dictionary for words.\n",
    "\n",
    "    > Use case:\n",
    "        1) Text Summarization\n",
    "        2) Language translation\n",
    "        3) Chat bot\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f680f82",
   "metadata": {},
   "source": [
    "## Text preprocessing 2 (Convert words to vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c305c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
